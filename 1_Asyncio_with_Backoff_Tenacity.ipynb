{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPouVJJBWNuSIGUyKqMotFX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nafis-neehal/new-tech-everyweek/blob/main/1_Asyncio_with_Backoff_Tenacity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learn Asyncio, and Backoff/Tenacity\n",
        "Asyncio - to do asynchronous programming, helpful when multiple independent LLM API calls and no need for sequential calls  \n",
        "Backoff/Tenacity - two libraries, to write decorator functions to deal with retrying upon failure"
      ],
      "metadata": {
        "id": "onwk9wGBoOuR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nUdSYwlOe7o6"
      },
      "outputs": [],
      "source": [
        "!pip3 install asyncio --quiet\n",
        "!pip3 install backoff --quiet\n",
        "!pip3 install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import openai\n",
        "import backoff\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "LW3IOdHnijOR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use of asyncio.gather()\n",
        "Helps with running multiple asynchronous tasks and collect their results once all tasks are finished"
      ],
      "metadata": {
        "id": "5gni932mpd6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def fetch_data(url):\n",
        "    # Simulate fetching data from a URL\n",
        "    # (In a real scenario, this would involve asynchronous I/O operations)\n",
        "    return f\"Data from {url}\"\n",
        "\n",
        "async def main():\n",
        "    tasks = ['url'+str(i) for i in range(100)]\n",
        "    results = [fetch_data(task) for task in tasks]\n",
        "    final_results = await asyncio.gather(*results)\n",
        "    print(final_results)\n",
        "\n",
        "asyncio.run(main())\n",
        "#await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-tFBmhYphTz",
        "outputId": "f2bf21b6-7bbc-4d5c-c428-39b1f472dd02"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Data from url0', 'Data from url1', 'Data from url2', 'Data from url3', 'Data from url4', 'Data from url5', 'Data from url6', 'Data from url7', 'Data from url8', 'Data from url9', 'Data from url10', 'Data from url11', 'Data from url12', 'Data from url13', 'Data from url14', 'Data from url15', 'Data from url16', 'Data from url17', 'Data from url18', 'Data from url19', 'Data from url20', 'Data from url21', 'Data from url22', 'Data from url23', 'Data from url24', 'Data from url25', 'Data from url26', 'Data from url27', 'Data from url28', 'Data from url29', 'Data from url30', 'Data from url31', 'Data from url32', 'Data from url33', 'Data from url34', 'Data from url35', 'Data from url36', 'Data from url37', 'Data from url38', 'Data from url39', 'Data from url40', 'Data from url41', 'Data from url42', 'Data from url43', 'Data from url44', 'Data from url45', 'Data from url46', 'Data from url47', 'Data from url48', 'Data from url49', 'Data from url50', 'Data from url51', 'Data from url52', 'Data from url53', 'Data from url54', 'Data from url55', 'Data from url56', 'Data from url57', 'Data from url58', 'Data from url59', 'Data from url60', 'Data from url61', 'Data from url62', 'Data from url63', 'Data from url64', 'Data from url65', 'Data from url66', 'Data from url67', 'Data from url68', 'Data from url69', 'Data from url70', 'Data from url71', 'Data from url72', 'Data from url73', 'Data from url74', 'Data from url75', 'Data from url76', 'Data from url77', 'Data from url78', 'Data from url79', 'Data from url80', 'Data from url81', 'Data from url82', 'Data from url83', 'Data from url84', 'Data from url85', 'Data from url86', 'Data from url87', 'Data from url88', 'Data from url89', 'Data from url90', 'Data from url91', 'Data from url92', 'Data from url93', 'Data from url94', 'Data from url95', 'Data from url96', 'Data from url97', 'Data from url98', 'Data from url99']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use of asyncio.completed()\n",
        "Handy for handling tasks as they are completed, allowing you to process results as soon as they are available."
      ],
      "metadata": {
        "id": "SUCPn1e6rvBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def fetch_data(url):\n",
        "    # Simulate fetching data from a URL\n",
        "    # (In a real scenario, this would involve asynchronous I/O operations)\n",
        "    return f\"Data from {url}\"\n",
        "\n",
        "async def main():\n",
        "    tasks = ['url '+str(i) for i in range(10)]\n",
        "    results = [fetch_data(task) for task in tasks]\n",
        "    for completed_task in asyncio.as_completed(results):\n",
        "      result = await completed_task\n",
        "      print(result)\n",
        "\n",
        "asyncio.run(main())\n",
        "#await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FecjKpxVr2XG",
        "outputId": "4111e3f4-877b-450e-a657-e77a5e3bd344"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data from url 6\n",
            "Data from url 3\n",
            "Data from url 7\n",
            "Data from url 5\n",
            "Data from url 8\n",
            "Data from url 4\n",
            "Data from url 9\n",
            "Data from url 0\n",
            "Data from url 2\n",
            "Data from url 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OpenAI Chat Completion with Async Code with Backoff and asyncio.gather()\n",
        "\n",
        "The order of the questions and answers are same. Gather() collects all the results first then processes it."
      ],
      "metadata": {
        "id": "EoYRENZ9jKoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@backoff.on_exception(backoff.expo, openai.RateLimitError)\n",
        "async def make_api_call_to_gpt(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = await client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "madNY8HhjVXQ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "  prompts = [\"Who is the first president of USA?\", \"Describe in 2 lines how does Async programming work?\", \"What is your greatest fear?\", \"When is 0=1?\", \"How many 'r' is there in the word 'Strawberry'?\"]\n",
        "  results = [make_api_call_to_gpt(prompt) for prompt in prompts]\n",
        "  final_results = await asyncio.gather(*results)\n",
        "  for result in final_results:\n",
        "    print(result)\n",
        "\n",
        "# asyncio.run(main()) <- an event loop is already running by running this jupyter cell, so this will throw an error\n",
        "await main() # <- use this instead to run the main function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU7aA8KVjnxP",
        "outputId": "9cbd0166-e8dd-4032-a648-fd39b1e15d3f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "George Washington\n",
            "Async programming allows tasks to run concurrently, without blocking the main thread. It uses callbacks or promises to handle the completion of tasks asynchronously.\n",
            "As an AI, I do not have personal fears or emotions.\n",
            "In mathematics, the equation 0=1 is never true. This is because 0 and 1 are distinct numbers with different values, and they cannot be equal to each other.\n",
            "There are 2 'r's in the word 'Strawberry'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI Chat Completion\n",
        "\n",
        "You will find that the order of the questions and the order of the answers printed are different - proving the asynchronous calls. As_completed() processes the results as soon as they are complete."
      ],
      "metadata": {
        "id": "qXJZbeBljqZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import openai\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def chat_completion(client, prompt, **kwargs):\n",
        "  messages = [{\"role\":\"user\", \"content\": prompt}]\n",
        "  return await client.chat.completions.create(messages=messages, **kwargs)\n",
        "\n",
        "async def run_chat_completions(client, prompts, **kwargs):\n",
        "  calls = [chat_completion(client, prompt, **kwargs) for prompt in prompts]\n",
        "  for completed_task in asyncio.as_completed(calls): # each completed task is an API call\n",
        "    response = await completed_task\n",
        "    print(response.choices[0].message.content)\n",
        "\n",
        "async def main(prompts, **kwargs):\n",
        "  #ensures that the client is automatically closed when it’s no longer needed, preventing resource leaks from occurring\n",
        "  async with openai.AsyncOpenAI(api_key = userdata.get('OPENAI_KEY_BS1')) as client:\n",
        "    await run_chat_completions(client, prompts, **kwargs)\n",
        "\n",
        "prompts = [\"Who is the first president of USA?\", \"Describe in 2 lines how does Async programming work?\", \"What is your greatest fear?\", \"When is 0=1?\", \"How many 'r' is there in the word 'Strawberry'?\"]\n",
        "\n",
        "asyncio.run(main(prompts, model=\"gpt-3.5-turbo\", temperature=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pai6do5zjqme",
        "outputId": "23d4e0f9-b161-4277-8b62-7174eca60a12"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "George Washington\n",
            "There are two 'r's in the word \"Strawberry\".\n",
            "As an AI, I do not experience fear or emotions.\n",
            "In mathematics, 0 does not equal 1. The statement \"0=1\" is always false in standard arithmetic.\n",
            "Async programming allows tasks to run concurrently, without blocking the main thread. It uses callbacks or promises to handle the completion of tasks asynchronously.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batching Requests\n",
        "This code will run the 6 requests in two batches, meaning it will wait for the first 2 prompts to finish running the next 2."
      ],
      "metadata": {
        "id": "Cp6p_SsdquJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batches(calls, batch_size):\n",
        "    for i in range(0, len(calls), batch_size):\n",
        "        yield calls[i : i + batch_size]"
      ],
      "metadata": {
        "id": "Z-w-AFKbqteT"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def chat_completion(client, prompt, **kwargs):\n",
        "  messages = [{\"role\":\"user\", \"content\": prompt}]\n",
        "  return await client.chat.completions.create(messages=messages, **kwargs)\n",
        "\n",
        "async def run_chat_completions(client, prompts: str, **kwargs):\n",
        "    calls = [chat_completion(client, prompt, **kwargs) for prompt in prompts]\n",
        "    for idx, batch in enumerate(generate_batches(calls, batch_size = 3)):\n",
        "        print(f\"Batch No {idx+1}\")\n",
        "        print(\"----------------\")\n",
        "        for completed_task in asyncio.as_completed(batch):\n",
        "            response = await completed_task\n",
        "            print(response.choices[0].message.content)\n",
        "\n",
        "async def main(prompts, **kwargs):\n",
        "    async with openai.AsyncClient(api_key = userdata.get('OPENAI_KEY_BS1'), timeout=60) as client:\n",
        "        await run_chat_completions(client, prompts, **kwargs)\n",
        "\n",
        "prompts = [\"Who is the first president of USA?\", \"Who first landed on the Moon?\",\n",
        "           \"Describe in 2 lines how does Async programming work?\", \"What is your greatest fear?\",\n",
        "           \"When is 0=1?\", \"How many 'r' is there in the word 'Strawberry'?\"]\n",
        "\n",
        "asyncio.run(main(prompts, model=\"gpt-3.5-turbo\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb-ZlXhCq4XF",
        "outputId": "0e095daa-85bc-4589-cb05-5e0ae10a5055"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch No 1\n",
            "----------------\n",
            "George Washington\n",
            "The first person to land on the Moon was American astronaut Neil Armstrong on July 20, 1969, as part of the Apollo 11 mission. He was followed shortly after by astronaut Edwin \"Buzz\" Aldrin.\n",
            "Async programming allows code to run non-sequentially, performing tasks concurrently without blocking the main execution thread. It uses callbacks, promises, or async/await keywords to handle asynchronous operations and ensure smooth, responsive application performance.\n",
            "Batch No 2\n",
            "----------------\n",
            "There are three 'r's in the word \"Strawberry.\"\n",
            "As an AI, I do not possess emotions or fear.\n",
            "In mathematics, the statement \"0=1\" is never true. This is because 0 and 1 are two distinct numbers with different values and properties. In mathematical equations and expressions, it is not possible for 0 to equal 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tenacity decorators for error handling and retrying with constraints\n",
        "\n",
        "This code will re-run any requests that raise one of the below-mentioned errors (see openai errors for more detail) up to 3 times, increasing the waiting time for each iteration exponentially starting from 2sec up to 5sec."
      ],
      "metadata": {
        "id": "PRZltWTCuv9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from openai import AsyncOpenAI, APITimeoutError, InternalServerError, RateLimitError, UnprocessableEntityError\n",
        "from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(min=2, max=5),\n",
        "    retry=retry_if_exception_type(\n",
        "        (\n",
        "            APITimeoutError,\n",
        "            InternalServerError,\n",
        "            RateLimitError,\n",
        "            UnprocessableEntityError\n",
        "        )\n",
        "    )\n",
        ")\n",
        "async def chat_completion(client, prompt, **kwargs):\n",
        "  messages = [{\"role\":\"user\", \"content\": prompt}]\n",
        "  return await client.chat.completions.create(messages=messages, **kwargs)\n",
        "\n",
        "async def run_chat_completions(client, prompts, **kwargs):\n",
        "  calls = [chat_completion(client, prompt, **kwargs) for prompt in prompts]\n",
        "  for completed_task in asyncio.as_completed(calls): # each completed task is an API call\n",
        "    response = await completed_task\n",
        "    print(response.choices[0].message.content)\n",
        "\n",
        "async def main(prompts, **kwargs):\n",
        "  #ensures that the client is automatically closed when it’s no longer needed, preventing resource leaks from occurring\n",
        "  async with openai.AsyncOpenAI(api_key = userdata.get('OPENAI_KEY_BS1')) as client:\n",
        "    await run_chat_completions(client, prompts, **kwargs)\n",
        "\n",
        "prompts = [\"Who is the first president of USA?\", \"Describe in 2 lines how does Async programming work?\", \"What is your greatest fear?\", \"When is 0=1?\", \"How many 'r' is there in the word 'Strawberry'?\"]\n",
        "\n",
        "asyncio.run(main(prompts, model=\"gpt-3.5-turbo\", temperature=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtzTzIU1uXsA",
        "outputId": "651c12d3-f334-4fec-cdb4-7e4385b401c2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "George Washington\n",
            "As an AI, I do not have personal fears or emotions.\n",
            "There are two 'r's in the word \"Strawberry\".\n",
            "Async programming allows tasks to run concurrently, without blocking the main thread. It uses callbacks or promises to handle the completion of tasks asynchronously.\n",
            "In mathematics, the equation 0=1 is never true. This is because 0 and 1 are distinct numbers with different values, and they cannot be equal to each other.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "\n",
        "*   https://diverger.medium.com/building-asynchronous-llm-applications-in-python-f775da7b15d1\n",
        "\n"
      ],
      "metadata": {
        "id": "7BV-pKbBqYsf"
      }
    }
  ]
}